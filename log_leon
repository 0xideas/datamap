LOG datamap project - Leon

08-11-17

9:30 initial meeting
11:00 setting up folder structure & learning git
<<<<<<< HEAD
12:30 look at character-level RNN implementation
13:00

09-11-17

16:30 learning git
16:50 look at tensorflow implementation of LSTM:
	https://www.tensorflow.org/tutorials/recurrent
	http://colah.github.io/posts/2015-08-Understanding-LSTMs/
	http://colah.github.io/posts/2014-07-Conv-Nets-Modular/
	http://colah.github.io/posts/2014-07-Understanding-Convolutions/
18:30

10-11-17

11:00 implementing lstm from scratch:
	https://github.com/nicodjimenez/lstm/blob/master/lstm.py
	https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
12:00 look at GDPR data types
12:30 create prioritisation table
13:30

14:45 prioritisation table
16:00 strategy 1
17:00 research
	https://en.wikipedia.org/wiki/Ontology_%28information_science%29
	https://hackernoon.com/wtf-is-a-knowledge-graph-a16603a1a25f
19:20

14-11-17

13:00 meeting 2 with Thomas & Toon
	https://medium.com/@acrosson/extracting-names-emails-and-phone-numbers-5d576354baa
13:30

15-11-17

15:30 setting up python environment

16:30

16-11-17
12:30 fighting my computer
13:45 trying out the code from above
14:30 research into NER
	http://www.opener-project.eu/project/performance/ - dutch, french, english, german
		language recognition
		tokeniser
		NER
	NLTK does not support languages other than English
	other multi-lingual POS tagger:
	
	http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/
	http://rdrpostagger.sourceforge.net/
15:30 break
16:00 learn regex
	https://www.regular-expressions.info/quickstart.html

17:00

17-11-17
12:20 create regular expressions
	https://regex101.com/r/dzDqmM/1
		regex checker
	http://www.xe.com/ibancalculator/sample/?ibancountry=belgium
		bank formats for belgium
	regexs done
13:00
14:00 writing extraction functions
	https://opendata.stackexchange.com/questions/4517/obtaining-personal-mail-corpus
		where to find email corpora
16:00 writing sample generation functions
17:00 testing extraction funcitons on sample
	PROBLEMS
	- failed to import functions 
		works now for some reason
	- standardising results in a fail
		solved by changing one regex so that it does not return groups
	- regular expressions may not be specific enough
18:00 creating a text file from the whole enron cache
	PROBLEM
	- UnicodeDecodeError: 'utf-8' codec can't decode byte
		added unidecoder now but it still doesn't work
19:10 fight git
19:20
	
21-11-17 - notes

Mixture of softmaxes
http://smerity.com/articles/2017/mixture_of_softmaxes.html
https://arxiv.org/pdf/1711.03953.pdf

22-11-17
13:00 clarifying strategy
13:20 dealing with utf-8 decoding error in generate_write_sample.py
14:30 extract text from enron corpus
14:45 review regular expressions
15:50 break
16:20 ran out of RAM in file building process
16:30 found ncdu to assess disk usage of a folder
16:40 researching distributed representations of documents
	overview, but not for vector space models: The State of the Art in Semantic Representation
	fancy: Hierarchical Attention Networks for Document Classification - http://www.aclweb.org/anthology/N16-1174
	fast: Bag of Tricks for Efficient Text Classification
18:00 dividing enron_mail into batches
18:00 further research into fastText
	https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/
18:55 updating regexs in python file
19:00

23-11-17
10:50 continue creation of text files from enron dataset
10:55 adding custom customer ID regex input to return_IDs.py
11:50 rewriting the whole return_IDs.py to be more modular
13:30 break
14:05 debugging rewritten return_IDs.py
15:00 testing IID extraction on batch 26
16:00 it works!
16:05 researching name extraction from file
	http://timmcnamara.co.nz/post/2650550090/extracting-names-with-6-lines-of-python-code
	http://nlpforhackers.io/named-entity-extraction/
17:10 

24-11-17
09:00 project meeting
10:30 break
11:10 schedule:
	create master file of all files 
	make sure the code is well documented
	upload files on bitbucket
	contact Said
	learn nltk
12:20 break
13:20 check if code works as intended
14:10 upload to bitbucket2

29-11-17
10:30 review Thomas' code
10:50 rewrite regex functions
12:35 break
13:35 recognising names
	spacy is the best option
14:30 install spacy
15:10 break
15:30 debug installation
16:00 play around with spacy
	really excellent
17:00 

30-11-17
09:30 meeting Toon, Saaid
11:30 draw overview charts
12:20 break
13:05 try out spacy on larger files
	ca 2 seconds /10k words
14:00 write function
14:45 test function
15:05 done

01-12-17
11:00 write a topic detection algorithm
	research & planning
12:40
14:20 working on classification algorithm structure
15:45 researching fasttext
	http://ruder.io/word-embeddings-2017/
	pre-trained models in many languages: https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
17:50

06-12-17
12:00 learn to use fastText 
	datasets: http://ana.cachopo.org/datasets-for-single-label-text-categorization
13:00 break
13:30 train word vectorisation on Reuters dataset
	failed multiple times, takes loads of RAM & CPU
15:30 train text classifier on Reuters dataset
	added '__label__' to label coloumn
16:40

07-12-17
11:00 program planning
12:00 writing regex input functions
13:20 break
13:40 continue
16:30 write settings input functions
18:40

08-12-17
10:30 plan over next few days
	draw program plans
	write integrated program
	write unit tests
10:35 create program plan
13:20 break
13:50 procrastination
14:15 commenting, debugging yesterday's work
19:15 done

Worked 13-12 full time
Worked 14-12 full time
Worked 15-12 full time

08-01-18
10:20 Catch up
10:40 Learn OOP
13:20 break
14:00 continued
17:25

09-01-18
10:30 finish videos
13:20 break
14:00 finish videos
14:20 clean up settings input
18:40

10-01-18
10:00 reestablish bitbucket
10:30 check out Marouans code
11:20 consolidate code and write docstrings
13:10
























